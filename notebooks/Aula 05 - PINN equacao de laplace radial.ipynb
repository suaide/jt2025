{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5c4462",
   "metadata": {},
   "source": [
    "\n",
    "# PINN com Keras/TensorFlow: Equação de Laplace radial (região externa)\n",
    "## Alexandre Suaide\n",
    "\n",
    "Este notebook resolve com uma **rede neural informada por física (PINN)** a equação de Laplace radial em 2D na região\n",
    "**externa** de um cilindro de raio `R` com potencial fixo `V(R)=V0`.  \n",
    "Precisamos definir um raio `R_ext` com potencial fixo `V(R_ext) = V_ext` como condição de contorno, já que o problema tem que ser finito.\n",
    "\n",
    "**Equação (R ≤ r ≤ R_ext):**  \n",
    "$$ \\frac{1}{r} \\frac{d}{dr} ( r \\frac{dV}{dr} ) = 0 $$\n",
    "\n",
    "**Condições de contorno:**  \n",
    "- `V(R) = V0`  \n",
    "- `V(R_ext) = V_ext`  \n",
    "\n",
    "A solução analítica é do tipo:  \n",
    "$$ V(r) = A \\ln(r) +B $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251faa7-683f-4370-ae7b-d241da06ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se estiver no Google Colab, descomente a linha abaixo para instalar dependências:\n",
    "# !pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa os módulos necessários\n",
    "# dependendo da versão do TensoFlow, aparece um monte de warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fdaaa5-231e-4e5f-bf74-5569385c9b02",
   "metadata": {},
   "source": [
    "---\n",
    "### Parâmetros físicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# condições de contorno. Define o raio o cilindro, o raio externo e os potenciais\n",
    "\n",
    "R = 1.0\n",
    "V0 = 5.0\n",
    "R_ext = 5.0\n",
    "V_ext = 0.0\n",
    "\n",
    "# Amostragem dos pontos\n",
    "\n",
    "N_interior = 2000\n",
    "N_bc_R = 200\n",
    "N_bc_Rext = 200\n",
    "\n",
    "# Amostras no interior, entre o cilindro e o raio externo\n",
    "r_interior = np.random.uniform(R, R_ext, (N_interior,1)).astype(np.float32)\n",
    "\n",
    "# Valores nas Condições de contorno\n",
    "r_bc_R = np.full((N_bc_R,1), R, dtype=np.float32)\n",
    "r_bc_Rext = np.full((N_bc_Rext,1), R_ext, dtype=np.float32)\n",
    "V_R_target = np.full((N_bc_R,1), V0, dtype=np.float32)\n",
    "V_Rext_target = np.full((N_bc_Rext,1), V_ext, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa75bf0-1abd-4360-9e55-649405aec6bd",
   "metadata": {},
   "source": [
    "---\n",
    "### constroi o modelo da rede. Serão quatro camadas \n",
    "ao invés de inicializar os pesos de forma totalmente aleatória, o glorot_uniform inicializa\n",
    "com uma distribuição uniforme. Dessa forma, o risco dos gradientes ficarem malucos é minimizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ad805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(width=64, depth=4):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(1,)))\n",
    "    for _ in range(depth-1):\n",
    "        model.add(layers.Dense(width, activation=tf.nn.tanh, kernel_initializer=\"glorot_uniform\"))\n",
    "    model.add(layers.Dense(1, activation=None))\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d488e671-7aae-4e43-a37f-bcaf946c44b7",
   "metadata": {},
   "source": [
    "---\n",
    "### laplace_residual(model, r) \n",
    "\n",
    "Essa função calcula o resíduo da equação de Laplace radial em coordenadas cilíndricas:\n",
    "\n",
    "$$ \\frac{1}{r} \\frac{d}{dr} ( r \\frac{dV}{dr} ) = 0 $$\n",
    "\n",
    "que, expandida, fica\n",
    "\n",
    "$$ \\frac{d^2V}{dr^2} + \\frac{1}{r}\\frac{dV}{dr} =0 $$\n",
    "\n",
    "Essa função retorna exatamente esse valor para um determinado valor de $r$.\n",
    "\n",
    "No TensorFlow, o tf.GradientTape é o mecanismo que grava operações para permitir cálculo automático de derivadas. O tape (a \"fita\" que grava as operações) é descartado assim que você chama .gradient. Ou seja, você só pode calcular um gradiente por vez com aquele tape. Quando se usa persistent = True, o tape não é descartado automaticamente após a primeira chamada. Isso permite calcular várias derivadas (gradientes sucessivos), como no caso de derivadas de ordem superior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_residual(model, r):\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch(r)\n",
    "        with tf.GradientTape() as tape1:\n",
    "            tape1.watch(r)\n",
    "            V = model(r)\n",
    "        dV_dr = tape1.gradient(V, r)\n",
    "    d2V_dr2 = tape2.gradient(dV_dr, r)\n",
    "    residual = d2V_dr2 + dV_dr/(r+1e-6)\n",
    "    return residual\n",
    "\n",
    "# Teste\n",
    "test_r = tf.constant([[1.5]], dtype=tf.float32)\n",
    "laplace_residual(model, test_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901de299-6001-42db-9783-e5c7a230e715",
   "metadata": {},
   "source": [
    "---\n",
    "### Cálculo da perda\n",
    "\n",
    "A perda é composta de três componentes:\n",
    "- o desvio em relação à condição de contorno em R\n",
    "- o desvio em relação à condição de contorno em R_ext\n",
    "- O desvio em relação à equação de Laplace\n",
    "\n",
    "As três perdas são somadas. Também retorna cada componente individualmente para comparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a120bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss():\n",
    "    # PDE interior\n",
    "    r_tf = tf.convert_to_tensor(r_interior)\n",
    "    res = laplace_residual(model, r_tf)\n",
    "    loss_pde = tf.reduce_mean(tf.square(res))\n",
    "\n",
    "    # BC em R\n",
    "    rR = tf.convert_to_tensor(r_bc_R)\n",
    "    V_R = model(rR)\n",
    "    loss_bc_R = tf.reduce_mean(tf.square(V_R - V_R_target))\n",
    "\n",
    "    # BC em R_ext\n",
    "    rRe = tf.convert_to_tensor(r_bc_Rext)\n",
    "    V_Rext = model(rRe)\n",
    "    loss_bc_Rext = tf.reduce_mean(tf.square(V_Rext - V_Rext_target))\n",
    "\n",
    "    return loss_pde + loss_bc_R + loss_bc_Rext, loss_pde, loss_bc_R, loss_bc_Rext\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091557e-eb89-414a-bf57-934098f8b9df",
   "metadata": {},
   "source": [
    "---\n",
    "### Treinamento da rede\n",
    "\n",
    "Descrição do código:\n",
    "1. Loop manual de treinamento (for ep in range(1, epochs+1):) Em vez de chamar model.fit(...), você mesmo controla cada época.\n",
    "2. Calcula a perda: compute_loss(). Isso é diferente de uma rede tradicional, em que a perda é só, por exemplo, MSE entre predição e dado. Aqui, você precisa combinar várias contribuições físicas + de contorno.\n",
    "3. Gradientes e atualização dos pesos:\n",
    "    1. tape.gradient(loss, model.trainable_variables) calcula o gradiente da perda em relação aos pesos.\n",
    "    2. optimizer.apply_gradients(...) aplica a atualização de pesos.\n",
    "4. Histórico: Salva cada componente da perda em history para depois plotar curvas de convergência.\n",
    "\n",
    "#### Por que não usar simplesmente model.fit(...)?\n",
    "\n",
    "O model.fit do Keras é pensado para aprendizado supervisionado clássico, onde você fornece X (entradas) e y (saídas alvo) e ele minimiza uma função de perda padrão (MSE, cross-entropy, etc.) entre model(X) e y.\n",
    "\n",
    "No caso das PINNs você não tem \"rótulos\" tradicionais (y). O que você tem são equações diferenciais e condições de contorno, que você traduz em termos de resíduos.A função de perda é composta manualmente (PDE + contornos).\n",
    "\n",
    "Logo, precisa implementar o passo de treinamento na mão usando GradientTape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8779a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, history, verbose = True):\n",
    "    print_every = epochs/5\n",
    "    for ep in range(1, epochs+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, lp, lR, lRe = compute_loss()\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "        history[\"loss\"].append(float(loss.numpy()))\n",
    "        history[\"pde\"].append(float(lp.numpy()))\n",
    "        history[\"bc_R\"].append(float(lR.numpy()))\n",
    "        history[\"bc_Rext\"].append(float(lRe.numpy()))\n",
    "    \n",
    "        if ep % print_every == 0 and verbose:\n",
    "            print(f\"Epoch {ep:5d} | loss={loss:.3e} | pde={lp:.3e} | bc_R={lR:.3e} | bc_Rext={lRe:.3e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc266a42-bebc-40b4-8662-6c58c254c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\"loss\": [], \"pde\": [], \"bc_R\": [], \"bc_Rext\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebd3c3-fb4c-4e14-961c-70dc42833290",
   "metadata": {},
   "source": [
    "---\n",
    "### Treina e mostra resultados evoluindo com treinamento. Comparação com solução analítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0681bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solução analítica\n",
    "A = (V_ext - V0) / (np.log(R_ext) - np.log(R))\n",
    "B = V0 - A*np.log(R)\n",
    "\n",
    "r_plot = np.linspace(R, R_ext, 400).reshape(-1,1).astype(np.float32)\n",
    "V_true = A*np.log(r_plot) + B\n",
    "\n",
    "epochs = [10,40,150,200,200]\n",
    "total = 0\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for epoch in epochs:\n",
    "    train(epoch,history,False)\n",
    "    total = total + epoch\n",
    "    V_pred = model(r_plot).numpy()\n",
    "    plt.plot(r_plot, V_pred, label=f\"Modelo depois de {total} épocas\")\n",
    "\n",
    "plt.plot(r_plot, V_true, \"--\", label=\"Solução analítica\")\n",
    "plt.xlabel(\"r\")\n",
    "plt.ylabel(\"V(r)\")\n",
    "plt.title(\"Potencial radial (Laplace externo)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Curva de perda\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.yscale('log')\n",
    "plt.plot(history[\"loss\"], label=\"loss total\")\n",
    "plt.plot(history[\"pde\"], label=\"loss PDE\")\n",
    "plt.plot(history[\"bc_R\"], label=\"loss BC r=R\")\n",
    "plt.plot(history[\"bc_Rext\"], label=\"loss BC r=R_ext\")\n",
    "plt.xlabel(\"época\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Histórico de treinamento\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b85912-fffd-4f35-94b8-d8b27528278d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
